{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonathas-1993/Topicos_UFAM/blob/main/atividades/4_TextVectorization_tf_idf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ============================================================\n",
        "# 🔍 Análise de Sentimentos no Dataset IMDB usando TF-IDF\n",
        "# Autor: Jonathas Tavares Neves\n",
        "# Contexto: Aula de Aprendizado de Máquina\n",
        "# ============================================================\n",
        "# Objetivo:\n",
        "# Demonstrar passo a passo:\n",
        "# - Download e preparação do dataset IMDB\n",
        "# - Separação em treino, validação e teste\n",
        "# - Vetorização de texto usando TF-IDF\n",
        "# - Treinamento de um modelo de rede neural densa\n",
        "# - Avaliação e registro de resultados de acurácia\n",
        "# - Atividade de sala: cada aluno registra suas próprias simulações"
      ],
      "metadata": {
        "id": "UyzD7AnYEZZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhfu4wLzDMW8"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# ============================================================\n",
        "# 🧠 Importação das Bibliotecas\n",
        "# ============================================================\n",
        "\n",
        "# Bibliotecas para manipulação de arquivos e diretórios\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Bibliotecas para construção e treinamento de modelos\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Biblioteca para registrar resultados em tabela\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ-52T1SDMXE",
        "outputId": "59b0ea7e-9bb4-428e-c405-6588c3b47592"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  35.4M      0  0:00:02  0:00:02 --:--:-- 35.4M\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# ============================================================\n",
        "# 📦 Download e Extração do Dataset IMDB\n",
        "# ============================================================\n",
        "\n",
        "# Baixa o dataset IMDB (resenhas de filmes rotuladas)\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "\n",
        "# Descompacta o arquivo\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "\n",
        "# Remove os reviews não rotulados\n",
        "!rm -r aclImdb/train/unsup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57990e65"
      },
      "source": [
        "# %%\n",
        "# ============================================================\n",
        "# 🧩 Separação dos Dados de Treino e Validação (20% para validação)\n",
        "# ============================================================\n",
        "\n",
        "# Definindo diretórios principais\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "train_dir = base_dir / \"train\"\n",
        "val_dir = base_dir / \"val\"\n",
        "\n",
        "# Para cada categoria de sentimento (negativo/positivo)\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    # Cria a pasta de validação se não existir\n",
        "    os.makedirs(val_dir / category, exist_ok=True)\n",
        "\n",
        "    # Lista todos os arquivos da categoria no treino\n",
        "    files = os.listdir(train_dir / category)\n",
        "\n",
        "    # Embaralha os arquivos de forma reprodutível (seed fixa)\n",
        "    random.Random(1337).shuffle(files)\n",
        "\n",
        "    # Calcula 20% do total para validação\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "\n",
        "    # Seleciona os últimos 20% como validação\n",
        "    val_files = files[-num_val_samples:]\n",
        "\n",
        "    # Move os arquivos selecionados para a pasta de validação\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname, val_dir / category / fname)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O9GtKnaDMXH",
        "outputId": "3d6099e4-f358-4295-a24c-6ee5352668ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 16000 files belonging to 2 classes.\n",
            "Found 9000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# ============================================================\n",
        "# 📚 Criação dos Datasets TensorFlow\n",
        "# ============================================================\n",
        "\n",
        "batch_size = 32  # Quantidade de amostras processadas por vez\n",
        "\n",
        "# Dataset de treino\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    train_dir, batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Dataset de validação\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    val_dir, batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Dataset de teste\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    base_dir / \"test\", batch_size=batch_size\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_GOaLDLyDMXI"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# ============================================================\n",
        "# 🧮 Vetorização com TF-IDF\n",
        "# ============================================================\n",
        "\n",
        "max_tokens = 20000  # Limite de palavras no vocabulário\n",
        "\n",
        "# Cria vetor TF-IDF\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"tf_idf\"  # saída é vetor TF-IDF\n",
        ")\n",
        "\n",
        "# Criar dataset só com textos de treino\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "\n",
        "# Ajusta o vocabulário TF-IDF\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "# Aplica TF-IDF nos datasets\n",
        "tfidf_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "tfidf_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "tfidf_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "h8UeBO9ODMXJ",
        "outputId": "6a568779-e5d5-49f9-9084-7b3c311f5288"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20000\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │       \u001b[38;5;34m320,016\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">320,016</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# %%\n",
        "# ============================================================\n",
        "# 🧩 Definição do Modelo de Rede Neural\n",
        "# ============================================================\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    \"\"\"\n",
        "    Cria modelo neural simples:\n",
        "    - Entrada: vetor TF-IDF\n",
        "    - Camada oculta densa + ReLU\n",
        "    - Dropout para regularização\n",
        "    - Saída sigmoid para classificação binária\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\",\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Testa o modelo\n",
        "model = get_model(max_tokens=max_tokens)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XuruC8mnDMXK"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# ============================================================\n",
        "# 💾 Função de Treino e Registro de Resultados\n",
        "# ============================================================\n",
        "\n",
        "# Cria DataFrame para armazenar resultados das simulações\n",
        "resultados = pd.DataFrame(columns=[\"Épocas\", \"Dimensão Oculta\", \"Dropout\", \"Acurácia (Teste)\"])\n",
        "\n",
        "def rodar_simulacao(epochs=5, hidden_dim=16, dropout=0.5):\n",
        "    \"\"\"\n",
        "    Função para treinar modelo com parâmetros específicos e registrar\n",
        "    automaticamente a acurácia final no DataFrame global 'resultados'.\n",
        "    \"\"\"\n",
        "    global resultados  # Permite atualizar DataFrame externo\n",
        "\n",
        "    # Cria modelo com número de neurônios na camada oculta\n",
        "    model = get_model(max_tokens=max_tokens, hidden_dim=hidden_dim)\n",
        "\n",
        "    # Atualiza taxa de dropout (camada Dropout)\n",
        "    model.layers[2].rate = dropout\n",
        "\n",
        "    # Callback para salvar melhores pesos\n",
        "    checkpoint_filepath = '/tmp/checkpoint.weights.h5'\n",
        "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        save_best_only=True\n",
        "    )\n",
        "\n",
        "    # Treina o modelo\n",
        "    history = model.fit(\n",
        "        tfidf_train_ds.cache(),\n",
        "        validation_data=tfidf_val_ds.cache(),\n",
        "        epochs=epochs,\n",
        "        callbacks=[checkpoint_cb],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Carrega os melhores pesos\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "\n",
        "    # Avalia no conjunto de teste\n",
        "    test_loss, test_acc = model.evaluate(tfidf_test_ds, verbose=0)\n",
        "\n",
        "    # Adiciona resultados ao DataFrame\n",
        "    resultados = pd.concat([resultados, pd.DataFrame([{\n",
        "        \"Épocas\": epochs,\n",
        "        \"Dimensão Oculta\": hidden_dim,\n",
        "        \"Dropout\": dropout,\n",
        "        \"Acurácia (Teste)\": round(test_acc, 3)\n",
        "    }])], ignore_index=True)\n",
        "\n",
        "    # Mostra resultado desta simulação\n",
        "    print(f\"✅ Simulação concluída: Épocas={epochs}, Hidden={hidden_dim}, Dropout={dropout} -> Acurácia Teste={test_acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjAPiq_9DMXM",
        "outputId": "5af312ab-415b-40e8-b434-0661e6d7cbd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - accuracy: 0.7727 - loss: 0.4846 - val_accuracy: 0.9018 - val_loss: 0.2749\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9038 - loss: 0.2594 - val_accuracy: 0.8994 - val_loss: 0.2963\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9224 - loss: 0.2174 - val_accuracy: 0.8962 - val_loss: 0.3110\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9325 - loss: 0.1965 - val_accuracy: 0.8984 - val_loss: 0.3277\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9419 - loss: 0.1769 - val_accuracy: 0.8948 - val_loss: 0.3543\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d58bf398bf0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# %%\n",
        "# =========================================\n",
        "# Treinamento\n",
        "# =========================================\n",
        "model.fit(\n",
        "    tfidf_train_ds.cache(),\n",
        "    validation_data=tfidf_val_ds.cache(),\n",
        "    epochs=5,\n",
        "    callbacks=[model_checkpoint_callback]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LzBGcv9DMXO",
        "outputId": "b310ba2d-73b7-497a-f4eb-d77f8addb876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.8892 - loss: 0.2948\n",
            "Test accuracy: 0.888\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# =========================================\n",
        "# Avaliação com os melhores pesos\n",
        "# =========================================\n",
        "model.load_weights(checkpoint_filepath)\n",
        "test_loss, test_acc = model.evaluate(tfidf_test_ds)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}